{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. 필요 라이브러리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# install those if needed\n",
    "\n",
    "# pip install datasketch\n",
    "# pip install networkx\n",
    "# pip install fuzzywuzzy\n",
    "# pip install datasets\n",
    "\n",
    "import os\n",
    "from tqdm.notebook import tqdm\n",
    "from datasketch import MinHash, MinHashLSH          # MinHash와 LSH를 utilize하기 위한 라이브러리 \n",
    "import networkx as nx                               # documents pairs를 documents cluster로 만들어주기 위한 그래프 관련 라이브러리\n",
    "from fuzzywuzzy import fuzz                         # levenshtein 거리를 구하기 위한 라이브러리\n",
    "from datasets import Dataset, load_dataset, get_dataset_config_names"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Deduplicate를 위한 MinHash와 LSH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSHSubSet():\n",
    "    \"\"\"\n",
    "    주어진 문서의 MinHash 값을 계산하고 해쉬 백터를 b개의 버킷으로 나누어 각각을 문서의 key 값으로 사용.\n",
    "    이 때, b는 MinHash의 Threshold Jaccard Sim.을 기준으로 정함.\n",
    "    같은 버킷에 들어 있는 documents들에 대해 filtering threshold를 넘는지 계산한 후, 넘으면 dedup.\n",
    "\n",
    "    :param doc_list: Dataset\n",
    "    :param num_perm: MinHash의 차원\n",
    "    :param seed: MinHash를 구할 때 사용할 랜덤 시드 값\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, doc_list, num_perm = 128, seed = 42):\n",
    "        self.doc_list = doc_list\n",
    "        self.num_perm = num_perm\n",
    "        self.seed = seed\n",
    "        self.doc_list_inverse = {doc:idx for idx, doc in enumerate(doc_list)}\n",
    "\n",
    "    def _preprocess(self):\n",
    "        \"\"\"\n",
    "        input document를 shingles(n-gram)로 변환.\n",
    "        shingles 기반의 Jaccard Sim.을 기준으로 signiture hash(MinHash)값이 정해짐.\n",
    "        baseline은 간단히 split-unigram으로 구현\n",
    "        \"\"\"\n",
    "\n",
    "        self.doc_dict = {document:document.split() for document in self.doc_list}\n",
    "\n",
    "    def _create_cand_pairs(self, lsh, min_hashes):\n",
    "        \"\"\"\n",
    "        계산된 MinHash를 참조, 각 document들에 대해 Threshold Jaccard Sim.을 넘는 유사한 문서들을 pair로 return.\n",
    "\n",
    "        \n",
    "        :param lsh: 전체 문서들에 대한 MinHash table\n",
    "        :param min_hashes: Original문서를 key로, signiture hash값을 value로 가지고 있는 MinHash 객체\n",
    "\n",
    "        :return: Dedup checking이 필요한 document pairs를 담고 있는 list\n",
    "        \"\"\"\n",
    "\n",
    "        no_check = []\n",
    "        need_check = []\n",
    "        sanity_symmetric = set()\n",
    "        for idx, min_hash in enumerate(min_hashes):\n",
    "            bucket = lsh.query(min_hash)\n",
    "            if len(bucket)==1:\n",
    "                no_check.append(idx)\n",
    "            elif len(bucket)>1:\n",
    "                first_val = self.doc_list[idx]\n",
    "                for val in bucket:\n",
    "                    if val == self.doc_list[idx]:\n",
    "                        continue\n",
    "                    second_val = val\n",
    "                    need_check.append([first_val,second_val])\n",
    "                    sanity_symmetric.add(self.doc_list_inverse[second_val])\n",
    "        no_check = [self.doc_list[idx] for idx in tqdm(no_check, desc='sanity symmetric...') if idx not in sanity_symmetric]\n",
    "        return no_check, need_check\n",
    "    \n",
    "    def _picked_by_graph(self, big_list):\n",
    "        \"\"\"\n",
    "        문서 pair를 graph로 변환 후, 비교할 set을 만들어 줌.\n",
    "        비교가 필요없는 sole node는 바로 final_docs list에 넣어줌.\n",
    "\n",
    "        \n",
    "        :param big_list: Dedup checking이 필요한 documents pairs를 담고 있는 list\n",
    "\n",
    "        :return: Dedup checking이 필요하지 않은 documents를 담고 있는 list, 서로서로 dedup checking을 해야하는 set을 담고 있는 list\n",
    "        \"\"\"\n",
    "\n",
    "        graph = []\n",
    "        for pair in tqdm(big_list, desc='Building graph...'):\n",
    "            graph.append(tuple(self.doc_list_inverse[doc] for doc in pair))\n",
    "        G = nx.Graph()\n",
    "        G.add_edges_from(graph)\n",
    "        return list(nx.connected_components(G))\n",
    "    \n",
    "    def _dedup_by_idx(self, doc_indices):\n",
    "        \"\"\"\n",
    "        Dedup checking 대상이 되는 한 cluster에 대해 one-by-one으로 dedup checking.\n",
    "        baseline은 reference에서 많이 사용하는 levenshtein ratio 0.8로 설정.\n",
    "        만약 어떤 pair가 dup이라면, 둘 중 긴 document를 제거(보통 긴 document는 불필요한 spamming을 담고 있는 경우가 많음).\n",
    "\n",
    "        :param big_list: Dedup checking이 필요한 documents set의 indices\n",
    "\n",
    "        :return: Dedup checking 후 남은 documents들의 indices, Dedup checking 후 제거된 documents들의 indices, \n",
    "        \"\"\"\n",
    "\n",
    "        doc_indices = sorted(doc_indices, key=lambda x: len(self.doc_list[x]))\n",
    "        final_indices = []\n",
    "        removed_indices = []\n",
    "\n",
    "        while doc_indices:\n",
    "            current_val = doc_indices.pop()\n",
    "            is_duplicate = False\n",
    "\n",
    "            for other_val in reversed(doc_indices):\n",
    "                if fuzz.ratio(self.doc_list[current_val], self.doc_list[other_val]) > 70:\n",
    "                    is_duplicate = True\n",
    "                    removed_indices.append((current_val, other_val))\n",
    "                    break\n",
    "\n",
    "            if not is_duplicate:\n",
    "                final_indices.append(current_val)\n",
    "\n",
    "        return final_indices, removed_indices     \n",
    "\n",
    "    def process(self):\n",
    "        self._preprocess()\n",
    "        print('Tokenizing Done.')\n",
    "        # Create LSH index \n",
    "        lsh = MinHashLSH(threshold=0.7, num_perm=self.num_perm)\n",
    "        min_hashes = []\n",
    "        for doc_ori, doc_prep in tqdm(self.doc_dict.items(), desc='Building Hash...'):\n",
    "            # min hash 계산\n",
    "            min_hash = MinHash(num_perm=self.num_perm, seed=self.seed)\n",
    "            for d in doc_prep:\n",
    "                min_hash.update(d.encode('utf8'))\n",
    "            lsh.insert(doc_ori, min_hash)\n",
    "            min_hashes.append(min_hash)\n",
    "        print('Building Hash Done.')\n",
    "        final_docs, pairs_list = self._create_cand_pairs(lsh, min_hashes)\n",
    "        print('Creating Pair Done.')\n",
    "        clusters = self._picked_by_graph(pairs_list)\n",
    "        print('Graphify Done.')\n",
    "        removed_docs = []\n",
    "        for doc_indices in tqdm(clusters, desc='Deduplicating...'):\n",
    "            final_indices, removed_indices = self._dedup_by_idx(doc_indices)\n",
    "            final_docs.extend([self.doc_list[idx] for idx in final_indices])\n",
    "            removed_docs.extend([(self.doc_list[removed_idx], self.doc_list[compared_idx]) for removed_idx, compared_idx in removed_indices])\n",
    "        print('Jobs All Done.')\n",
    "\n",
    "        return final_docs, removed_docs\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. korean_textbooks deduplication"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_dataset_config_names(\"maywell/korean_textbooks\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASET_NAME = [i for i in get_dataset_config_names(\"maywell/korean_textbooks\")]\n",
    "# DATASET_NAME = [i for i in get_dataset_config_names(\"maywell/korean_textbooks\") if i =='helpsteer']\n",
    "print(DATASET_NAME)\n",
    "raw_corpus = [load_dataset(\"maywell/korean_textbooks\", dn) for dn in DATASET_NAME]\n",
    "deduped_corpus = []\n",
    "removed_corpus = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for dn, data in zip(DATASET_NAME, raw_corpus):\n",
    "    print(f'{dn} dedup ongoing...')\n",
    "    print(f\"{len(data := data['train']['text'])} samples.\")\n",
    "    raw_len = len(data)\n",
    "\n",
    "    lshsubset = LSHSubSet(data)\n",
    "    final_docs, removed_docs = lshsubset.process()\n",
    "\n",
    "    print(f'{dn} dedup done.')\n",
    "    print(f\"{raw_len} -> {len(final_docs)} samples.\")\n",
    "    print(\"-\"*10)\n",
    "    \n",
    "    deduped_corpus.append(final_docs)\n",
    "    removed_corpus.append(removed_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[('\"새로운 토론 주제\"에 대한 토론 내용:\\n\\n토론', '\"새로운 토론 주제\"에 대한 토론 내용:\\n\\n')]]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "removed_corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "deduped_corpus: deduplicated된 문서들 (List[List[str]])\n",
    "\n",
    "removed_corpus: 제거의 기준이 된 문서와 제거된 문서의 쌍들 (List[List[Tuple[str, str]]])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "WordExtraction",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
